{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13oXpzDXUfPv"
      },
      "source": [
        "Instructions\n",
        "\n",
        "1.   Install libraries in requirements.txt\n",
        "2.   Download the data train.h5 and val.h5 and update setup cell with the right path.\n",
        "3.   Hit Run All\n",
        "4. Remove collab specific instructions (those that start with !)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaPwBlAokPoW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import ViTImageProcessor, ViTModel, GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2Encoder\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from torchvision import transforms\n",
        "import h5py\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torch_geometric.nn import GATv2Conv,GCNConv\n",
        "from PIL import Image\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLD9kKwuSj2P"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv11AUWC4_ck",
        "outputId": "e77252a9-b9d1-4558-c553-7b6727160c2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 24\n",
        "NUM_EPOCHS = 15\n",
        "# Mount Google Drive\n",
        "\n",
        "\n",
        "# Define paths to datasets\n",
        "train_path = './data/train.h5'\n",
        "val_path = './data/val.h5'\n",
        "model_path = './models/multimodal_multilabel.pth'\n",
        "# Create dataset objects\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOvRF_gWTLe5"
      },
      "source": [
        "## Graph and base dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV2FvPpdxjil"
      },
      "outputs": [],
      "source": [
        "# Class Map\n",
        "\n",
        "label_map = {\n",
        "    \"Root\":0,\n",
        "    \"Logos\": 1,\n",
        "    \"Repetition\": 2,\n",
        "    \"Obfuscation, Intentional vagueness, Confusion\": 3,\n",
        "    \"Reasoning\": 4,\n",
        "    \"Justification\": 5,\n",
        "    \"Slogans\": 6,\n",
        "    \"Bandwagon\": 7,\n",
        "    \"Appeal to authority\": 8,\n",
        "    \"Flag-waving\": 9,\n",
        "    \"Appeal to fear/prejudice\": 10,\n",
        "    \"Simplification\": 11,\n",
        "    \"Causal Oversimplification\": 12,\n",
        "    \"Black-and-white Fallacy/Dictatorship\": 13,\n",
        "    \"Thought-terminating cliché\": 14,\n",
        "    \"Distraction\": 15,\n",
        "    \"Misrepresentation of Someone's Position (Straw Man)\": 16,\n",
        "    \"Presenting Irrelevant Data (Red Herring)\": 17,\n",
        "    \"Whataboutism\": 18,\n",
        "    \"Ethos\": 19,\n",
        "    \"Glittering generalities (Virtue)\": 20,\n",
        "    \"Ad Hominem\": 21,\n",
        "    \"Doubt\": 22,\n",
        "    \"Name calling/Labeling\": 23,\n",
        "    \"Smears\": 24,\n",
        "    \"Reductio ad hitlerum\": 25,\n",
        "    \"Pathos\": 26,\n",
        "    \"Exaggeration/Minimisation\": 27,\n",
        "    \"Loaded Language\": 28,\n",
        "    \"Transfer\": 29,\n",
        "    \"Appeal to (Strong) Emotions\":30\n",
        "}\n",
        "\n",
        "def get_label_index(label):\n",
        "    return label_map[label]\n",
        "\n",
        "def get_label_for_index(index):\n",
        "    for label, idx in label_map.items():\n",
        "        if idx == index:\n",
        "            return label\n",
        "    assert False, f\"Unknown index: {index}\"\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "# Class hierarchy graph\n",
        "# used by GAT to embed hierarchy into encoders\n",
        "\n",
        "G = DiGraph()\n",
        "# Add top-level categories\n",
        "G.add_node(\"Root\", index=0)\n",
        "G.add_edge(\"Root\", \"Logos\")\n",
        "G.add_edge(\"Logos\", \"Repetition\")\n",
        "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
        "G.add_edge(\"Logos\", \"Reasoning\")\n",
        "G.add_edge(\"Logos\", \"Justification\")\n",
        "G.add_edge('Justification', \"Slogans\")\n",
        "G.add_edge('Justification', \"Bandwagon\")\n",
        "G.add_edge('Justification', \"Appeal to authority\")\n",
        "G.add_edge('Justification', \"Flag-waving\")\n",
        "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Reasoning', \"Simplification\")\n",
        "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
        "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
        "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
        "G.add_edge('Reasoning', \"Distraction\")\n",
        "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
        "G.add_edge('Distraction', \"Whataboutism\")\n",
        "G.add_edge(\"Root\", \"Ethos\")\n",
        "G.add_edge('Ethos', \"Appeal to authority\")\n",
        "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
        "G.add_edge('Ethos', \"Bandwagon\")\n",
        "G.add_edge('Ethos', \"Ad Hominem\")\n",
        "G.add_edge('Ethos', \"Transfer\")\n",
        "G.add_edge('Ad Hominem', \"Doubt\")\n",
        "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
        "G.add_edge('Ad Hominem', \"Smears\")\n",
        "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
        "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
        "G.add_edge(\"Root\", \"Pathos\")\n",
        "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
        "G.add_edge('Pathos', \"Loaded Language\")\n",
        "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
        "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Pathos', \"Flag-waving\")\n",
        "G.add_edge('Pathos', \"Transfer\")\n",
        "\n",
        "# Separate edges by layer\n",
        "layer1_edges = [\n",
        "    (get_label_index(\"Root\"), get_label_index(\"Logos\")),\n",
        "    (get_label_index(\"Root\"), get_label_index(\"Ethos\")),\n",
        "    (get_label_index(\"Root\"), get_label_index(\"Pathos\")),\n",
        "]\n",
        "\n",
        "layer2_edges = [\n",
        "    (get_label_index(\"Logos\"), get_label_index(\"Repetition\")),\n",
        "    (get_label_index(\"Logos\"), get_label_index(\"Obfuscation, Intentional vagueness, Confusion\")),\n",
        "    (get_label_index(\"Logos\"), get_label_index(\"Reasoning\")),\n",
        "    (get_label_index(\"Logos\"), get_label_index(\"Justification\")),\n",
        "    (get_label_index(\"Ethos\"), get_label_index(\"Appeal to authority\")),\n",
        "    (get_label_index(\"Ethos\"), get_label_index(\"Glittering generalities (Virtue)\")),\n",
        "    (get_label_index(\"Ethos\"), get_label_index(\"Bandwagon\")),\n",
        "    (get_label_index(\"Pathos\"), get_label_index(\"Exaggeration/Minimisation\")),\n",
        "    (get_label_index(\"Pathos\"), get_label_index(\"Loaded Language\")),\n",
        "]\n",
        "\n",
        "layer3_edges = [\n",
        "    (get_label_index(\"Reasoning\"), get_label_index(\"Simplification\")),\n",
        "    (get_label_index(\"Reasoning\"), get_label_index(\"Distraction\")),\n",
        "    (get_label_index(\"Simplification\"), get_label_index(\"Causal Oversimplification\")),\n",
        "    (get_label_index(\"Distraction\"), get_label_index(\"Whataboutism\")),\n",
        "]\n",
        "\n",
        "layers = [layer1_edges, layer2_edges, layer3_edges]\n",
        "\n",
        "def depth_to_label(depth):\n",
        "    if depth == 0:\n",
        "        return  layer1_edges\n",
        "    elif depth == 1:\n",
        "        return layer2_edges\n",
        "    elif depth == 2:\n",
        "        return layer3_edges\n",
        "    else:\n",
        "        assert False, f\"Unknown depth: {depth}\"\n",
        "\n",
        "# Dataset that returns tokenized string and meme\n",
        "class HDF5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5_path, label_map, hierarchy, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_path (str): Path to the HDF5 file.\n",
        "            label_map (dict): Mapping from label text to index.\n",
        "            hierarchy (DiGraph): Directed graph representing the hierarchy.\n",
        "            transform (Callable, optional): Optional image transformation.\n",
        "        \"\"\"\n",
        "        self.hdf5_path = hdf5_path\n",
        "        self.label_map = label_map\n",
        "        self.hierarchy = hierarchy\n",
        "        self.transform = transform\n",
        "        self.hf = None  # File handler to be opened on-demand\n",
        "\n",
        "    def __len__(self):\n",
        "        with h5py.File(self.hdf5_path, 'r') as hf:\n",
        "            return hf['labels'].shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        global tokenizer\n",
        "        if self.hf is None:\n",
        "            self.hf = h5py.File(self.hdf5_path, 'r')\n",
        "        text = self.hf[\"text\"][idx].decode(\"utf-8\")\n",
        "        # Load data\n",
        "        # get tokens from tokenizer\n",
        "        text_token_ids, token_type_ids, attention_mask = tokenizer(text)\n",
        "        image = torch.tensor(self.hf['images'][idx], dtype=torch.float32)\n",
        "\n",
        "        # Decode and convert text labels to binary vector\n",
        "        label_bytes = self.hf['labels'][idx]\n",
        "        label_texts = label_bytes.decode(\"utf-8\").strip().split(\"<?>\")  # Decode bytes to string\n",
        "        label_vector = torch.zeros(len(self.label_map), dtype=torch.float32)\n",
        "\n",
        "        # Set leaf labels\n",
        "        leaf_indices = [self.label_map[label] for label in label_texts if label in self.label_map]\n",
        "\n",
        "        # Propagate labels up the hierarchy\n",
        "        all_indices = set(leaf_indices)\n",
        "        for leaf in label_texts:\n",
        "            if leaf in self.label_map:\n",
        "                node = leaf\n",
        "                while node in self.hierarchy:\n",
        "                    all_indices.add(self.label_map[node])\n",
        "                    parent_nodes = list(self.hierarchy.predecessors(node))\n",
        "                    if not parent_nodes or parent_nodes[0] == \"Root\":\n",
        "                        break\n",
        "                    node = parent_nodes[0]\n",
        "            elif leaf == \"\":\n",
        "              return text_token_ids,token_type_ids, text_attention_masks, image, label_vector\n",
        "            else:\n",
        "              assert leaf == \"Root\", f\"Unknown label: {label_texts}\"\n",
        "\n",
        "        # Set binary vector for all labels\n",
        "        for idx in all_indices:\n",
        "            label_vector[idx] = 1.0\n",
        "\n",
        "        # Apply optional image transformation\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return text_token_ids, token_type_ids, text_attention_masks, image, label_vector\n",
        "\n",
        "    def close(self):\n",
        "        if self.hf is not None:\n",
        "            self.hf.close()\n",
        "            self.hf = None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB1opndPThn1"
      },
      "source": [
        "## Hierarchy dataset with soft prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XUNr3wMGF8A"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Dataset that returns tokenized text that is appended with soft prompt and meme\n",
        "class HDPureDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5_path, label_map, hierarchy, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_path (str): Path to the HDF5 file.\n",
        "            label_map (dict): Mapping from label text to index.\n",
        "            hierarchy (DiGraph): Directed graph representing the hierarchy.\n",
        "            transform (Callable, optional): Optional image transformation.\n",
        "        \"\"\"\n",
        "        self.hdf5_path = hdf5_path\n",
        "        self.label_map = label_map\n",
        "        self.hierarchy = hierarchy\n",
        "        self.transform = transform\n",
        "        self.hf = None  # File handler to be opened on-demand\n",
        "        self.hierarchy_levels = 3\n",
        "        print(self.hierarchy_levels)\n",
        "\n",
        "    def construct_prompt(self):\n",
        "        \"\"\"\n",
        "        Construct the prompt based on the hierarchy depth.\n",
        "        \"\"\"\n",
        "        prompt_tokens = [\"[V{}] [MASK]\".format(i) for i in range(1, self.hierarchy_levels + 1)]\n",
        "        return \" \".join(prompt_tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        with h5py.File(self.hdf5_path, 'r') as hf:\n",
        "            return hf['labels'].shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.hf is None:\n",
        "            self.hf = h5py.File(self.hdf5_path, 'r')\n",
        "        text = self.hf[\"text\"][idx].decode(\"utf-8\")\n",
        "        # Tokenize the combined input\n",
        "        tokenized = tokenizer(text, padding=\"max_length\", max_length=128,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        text_token_ids = tokenized[\"input_ids\"].squeeze(0)\n",
        "        token_type_ids = tokenized[\"token_type_ids\"].squeeze(0)\n",
        "        text_attention_masks = tokenized[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        image = torch.tensor(self.hf['images'][idx], dtype=torch.float32)\n",
        "\n",
        "        # Decode and convert text labels to binary vector\n",
        "        label_bytes = self.hf['labels'][idx]\n",
        "        label_texts = label_bytes.decode(\"utf-8\").strip().split(\"<?>\")  # Decode bytes to string\n",
        "        label_vector = torch.zeros(len(self.label_map), dtype=torch.float32)\n",
        "\n",
        "        # Set leaf labels\n",
        "        leaf_indices = [self.label_map[label] for label in label_texts if label in self.label_map]\n",
        "\n",
        "        # Propagate labels up the hierarchy\n",
        "        all_indices = set(leaf_indices)\n",
        "        for leaf in label_texts:\n",
        "            if leaf in self.label_map:\n",
        "                node = leaf\n",
        "                while node in self.hierarchy:\n",
        "                    all_indices.add(self.label_map[node])\n",
        "                    parent_nodes = list(self.hierarchy.predecessors(node))\n",
        "                    if not parent_nodes or parent_nodes[0] == \"Root\":\n",
        "                        break\n",
        "                    node = parent_nodes[0]\n",
        "            elif leaf == \"\":\n",
        "              return text_token_ids, token_type_ids, text_attention_masks, image, label_vector\n",
        "            else:\n",
        "              assert leaf == \"Root\", f\"Unknown label: {label_texts}\"\n",
        "\n",
        "        # Set binary vector for all labels\n",
        "        for idx in all_indices:\n",
        "            label_vector[idx] = 1.0\n",
        "\n",
        "        # Apply optional image transformation\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return text_token_ids, token_type_ids, text_attention_masks, image, label_vector\n",
        "\n",
        "    def close(self):\n",
        "        if self.hf is not None:\n",
        "            self.hf.close()\n",
        "            self.hf = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw7QncaaTUYR"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3d2TNu_-Oqs",
        "outputId": "5f369cf5-637c-4f08-b885-2179bf134c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "3\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "train_dataset = HDPureDataset(train_path, label_map, G)\n",
        "val_dataset = HDPureDataset(val_path, label_map, G)\n",
        "best_f1_score_g = None\n",
        "best_thresholds = None\n",
        "# Create DataLoaders\n",
        "all_val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "all_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VpWNEs5O-T6"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA2NcyQxkUPY"
      },
      "outputs": [],
      "source": [
        "class TransformerFusion(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, num_heads=8, num_layers=6, ff_dim=512, dropout=0.3, mlp_hidden_dim=512):\n",
        "        super(TransformerFusion, self).__init__()\n",
        "        # Transformer layers for text and image separately\n",
        "        self.text_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.image_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Cross-attention mechanism\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "        # Learnable attention pooling\n",
        "        self.text_pool_query = nn.Parameter(torch.empty(1, input_dim))  # Query for text\n",
        "        self.image_pool_query = nn.Parameter(torch.empty(1, input_dim))  # Query for image\n",
        "        self.cross_pool_query = nn.Parameter(torch.empty(1, input_dim))  # Query for cross-attention\n",
        "\n",
        "        # Initialize pooling queries using Xavier Initialization\n",
        "        nn.init.xavier_uniform_(self.text_pool_query)\n",
        "        nn.init.xavier_uniform_(self.image_pool_query)\n",
        "        nn.init.xavier_uniform_(self.cross_pool_query)\n",
        "\n",
        "        # MLP for feature fusion\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(3 * input_dim, mlp_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.text_norm = nn.LayerNorm(input_dim)\n",
        "        self.image_norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def attention_pooling(self, features, query):\n",
        "        \"\"\"\n",
        "        Perform attention-based pooling.\n",
        "        Args:\n",
        "            features: Tensor of shape [seq_len, batch_size, input_dim]\n",
        "            query: Tensor of shape [1, input_dim]\n",
        "        Returns:\n",
        "            Pooled output: Tensor of shape [batch_size, input_dim]\n",
        "        \"\"\"\n",
        "        attn_weights = torch.softmax(torch.matmul(features.transpose(0, 1), query.T), dim=1)  # [batch_size, seq_len, 1]\n",
        "        pooled = torch.sum(attn_weights * features.transpose(0, 1), dim=1)  # [batch_size, input_dim]\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_features = self.text_norm(text_features)\n",
        "        image_features = self.image_norm(image_features)\n",
        "\n",
        "        # Self-attention\n",
        "        text_features = self.text_transformer(text_features)\n",
        "        image_features = self.image_transformer(image_features)\n",
        "\n",
        "        # Cross-attention\n",
        "        cross_attn_output, _ = self.cross_attn(\n",
        "            query=text_features,\n",
        "            key=image_features,\n",
        "            value=image_features\n",
        "        )\n",
        "\n",
        "        # Attention-based pooling\n",
        "        text_pooled = self.attention_pooling(text_features, self.text_pool_query)\n",
        "        image_pooled = self.attention_pooling(image_features, self.image_pool_query)\n",
        "        cross_attn_pooled = self.attention_pooling(cross_attn_output, self.cross_pool_query)\n",
        "\n",
        "        # Concatenate pooled features and fuse with MLP\n",
        "        combined_features = torch.cat([text_pooled, text_pooled, text_pooled], dim=-1)\n",
        "        fused_features = self.mlp(combined_features)\n",
        "\n",
        "        return fused_features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eBiZStZPBuQ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_IKa1116Iw"
      },
      "source": [
        "## Base model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-NgqDY9Te7R"
      },
      "source": [
        "This is a model without enhancements. It consists of embedder and an optional  transformer fusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhk73JqykXIU"
      },
      "outputs": [],
      "source": [
        "class MulticlassMemeClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, hierarchy):\n",
        "        super(MulticlassMemeClassifier, self).__init__()\n",
        "        # Text encoder: Pretrained BERT\n",
        "        self.text_encoder = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "        self.text_fc = nn.Sequential(nn.Dropout(0.1), nn.Linear(768, 768),\n",
        "                                      nn.Dropout(0.1))\n",
        "\n",
        "        # Image encoder: Pretrained ViT\n",
        "        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.image_fc = nn.Sequential(nn.Dropout(0.1),\n",
        "                                      nn.Linear(768, 768),\n",
        "                                      nn.Dropout(0.1))\n",
        "\n",
        "        # Fusion mechanism\n",
        "        self.fusion = TransformerFusion(input_dim=768, hidden_dim=hidden_dim)\n",
        "\n",
        "        # Create classifier dictionary for hierarchical nodes\n",
        "        self.classifier_dict = nn.ModuleDict({\n",
        "            node: nn.Linear(hidden_dim, 1) for node in hierarchy.nodes if node != \"Root\"\n",
        "        })\n",
        "\n",
        "        self.hierarchy = hierarchy\n",
        "\n",
        "    def forward(self, text_input_ids, token_type_ids, text_attention_mask, images):\n",
        "        text_hidden_states = self.text_encoder(\n",
        "            input_ids=text_input_ids,\n",
        "            attention_mask=text_attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        ).last_hidden_state\n",
        "\n",
        "        # Text encoding\n",
        "        text_features = self.text_fc(text_hidden_states)\n",
        "\n",
        "        # Image encoding\n",
        "        image_hidden_states = self.image_encoder(pixel_values=images).last_hidden_state\n",
        "        image_features = self.image_fc(image_hidden_states)\n",
        "\n",
        "        # Transpose to [seq_len, batch_size, input_dim]\n",
        "        text_features = text_features.permute(1, 0, 2)\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion(text_features, image_features)\n",
        "\n",
        "        # Outputs for each node in the hierarchy\n",
        "        outputs = {node: self.classifier_dict[node](fused_features) for node in self.hierarchy.nodes if node != \"Root\"}\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb6V3F1h1--B"
      },
      "source": [
        "## Heirarchy model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIagJ3kjToEb"
      },
      "source": [
        "This model is integrated with GAT and soft prompt tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z54BXtZP19L0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "# Verbalizer predictions\n",
        "        outputs = {}\n",
        "        for i in range(self.L):\n",
        "            hidden_state = pred_hidden_states[i]\n",
        "            label_indices = [self.label2idx[label] for label in self.labels_per_layer[i]]\n",
        "            logits = torch.matmul(hidden_state, self.label_embeddings[label_indices].t())\n",
        "            for idx, label in enumerate(self.labels_per_layer[i]):\n",
        "                outputs[label] = logits[:, idx].unsqueeze(-1)\n",
        "\n",
        "        # Image encoding\n",
        "        image_features = self.image_encoder(pixel_values=images).last_hidden_state\n",
        "        image_features = self.image_fc(image_features)\n",
        "        print(image_features.shape, text_features.shape)\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "        text_features = text_features.permute(1, 0, 2)\n",
        "        # Verbalizer predictions\n",
        "\"\"\"\n",
        "class HierarchyAwareClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, hierarchy, edge_levels, pretrained_model=\"microsoft/deberta-v3-base\", num_gat_layers=3):\n",
        "        super(HierarchyAwareClassifier, self).__init__()\n",
        "        self.hierarchy = hierarchy\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.L = len(edge_levels)  # Hierarchy depth\n",
        "\n",
        "        # Tokenizer and text encoder\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        self.text_encoder = AutoModel.from_pretrained(pretrained_model, output_hidden_states=True)\n",
        "        self.deberta_hidden_dim = self.text_encoder.config.hidden_size\n",
        "\n",
        "        # Add [Vi] tokens to tokenizer\n",
        "        self.template_tokens = [f\"[V{i}]\" for i in range(1, self.L + 1)]\n",
        "        self.tokenizer.add_tokens(self.template_tokens)\n",
        "\n",
        "        # Add [PRED] token to tokenizer\n",
        "        self.tokenizer.add_tokens([\"[PRED]\"])\n",
        "\n",
        "        # Resize text encoder embeddings for new tokens\n",
        "        self.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Initialize template embeddings [V1], [V2], ..., [VL] (randomly initialized, learned during training)\n",
        "        self.template_embeddings = nn.Parameter(torch.randn(self.L, self.deberta_hidden_dim))\n",
        "\n",
        "        # Initialize [PRED] token embedding using [MASK] token embedding\n",
        "        mask_token_id = self.tokenizer.mask_token_id\n",
        "        self.pred_embedding = nn.Parameter(\n",
        "            self.text_encoder.embeddings.word_embeddings.weight[mask_token_id].clone().detach()\n",
        "        )\n",
        "\n",
        "        # Label embeddings for each label in the hierarchy\n",
        "        self.labels = [node for node in hierarchy.nodes if node != \"Root\"]\n",
        "        self.num_labels = len(self.labels)\n",
        "        self.label2idx = {label: idx for idx, label in enumerate(self.labels)}\n",
        "        self.idx2label = {idx: label for label, idx in self.label2idx.items()}\n",
        "\n",
        "        # Assign levels to labels\n",
        "        self.levels = self.assign_levels()\n",
        "\n",
        "        # Labels per layer\n",
        "        self.labels_per_layer = [[] for _ in range(self.L)]\n",
        "        for label in self.labels:\n",
        "            level = self.levels[label]\n",
        "            if level < self.L:\n",
        "                self.labels_per_layer[level].append(label)\n",
        "\n",
        "        # Virtual label embeddings initialized randomly\n",
        "        self.label_embeddings = nn.Parameter(torch.randn(self.num_labels, self.deberta_hidden_dim))\n",
        "\n",
        "        # GAT for hierarchy injection\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(self.deberta_hidden_dim, self.deberta_hidden_dim, heads=1)\n",
        "            for _ in range(num_gat_layers)\n",
        "        ])\n",
        "\n",
        "        # Build edge index\n",
        "        self.edge_index = self.build_edge_index()\n",
        "        self.fusion_model = TransformerFusion(self.deberta_hidden_dim)\n",
        "        # Image encoder: Pretrained ViT\n",
        "        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.image_fc = nn.Sequential(nn.Dropout(0.1))\n",
        "\n",
        "        # Fusion mechanism\n",
        "        self.fusion_fc = nn.Linear(self.deberta_hidden_dim + self.image_encoder.config.hidden_size, self.deberta_hidden_dim)\n",
        "\n",
        "        # Classifiers for each node\n",
        "        self.classifier_dict = nn.ModuleDict({\n",
        "            node: nn.Linear(hidden_dim, 1) for node in self.labels\n",
        "        })\n",
        "        self.verbalizer_projection = torch.nn.Linear(14, self.deberta_hidden_dim)\n",
        "\n",
        "    def assign_levels(self):\n",
        "        # Assign levels to each label in the hierarchy using BFS\n",
        "        levels = {}\n",
        "        queue = [('Root', 0)]\n",
        "        visited = set()\n",
        "        while queue:\n",
        "            node, level = queue.pop(0)\n",
        "            if node in visited:\n",
        "                continue\n",
        "            visited.add(node)\n",
        "            levels[node] = level\n",
        "            for child in self.hierarchy.neighbors(node):\n",
        "                if child not in visited:\n",
        "                    queue.append((child, level + 1))\n",
        "        return levels\n",
        "\n",
        "    def build_edge_index(self):\n",
        "        # Build edge index for GAT\n",
        "        edge_index = [[], []]\n",
        "        node_indices = {label: idx for idx, label in enumerate(self.labels)}\n",
        "        for i in range(self.L):\n",
        "            node_indices[f'V{i+1}'] = self.num_labels + i\n",
        "\n",
        "        # Add hierarchy edges\n",
        "        for parent, child in self.hierarchy.edges():\n",
        "            if parent == \"Root\":\n",
        "                continue\n",
        "            if parent in node_indices and child in node_indices:\n",
        "                edge_index[0].append(node_indices[parent])\n",
        "                edge_index[1].append(node_indices[child])\n",
        "\n",
        "        # Add virtual node edges\n",
        "        for i in range(self.L):\n",
        "            vi_idx = node_indices[f'V{i+1}']\n",
        "            for label in self.labels_per_layer[i]:\n",
        "                label_idx = node_indices[label]\n",
        "                edge_index[0].extend([vi_idx, label_idx])\n",
        "                edge_index[1].extend([label_idx, vi_idx])\n",
        "\n",
        "        return torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "    def get_gat_embeddings(self):\n",
        "\n",
        "      num_template_embeddings = self.template_embeddings.size(0)  # L\n",
        "\n",
        "      num_nodes = self.edge_index.max().item() + 1\n",
        "      input_embeddings = self.template_embeddings\n",
        "      if num_template_embeddings < num_nodes:\n",
        "          num_missing_nodes = num_nodes - num_template_embeddings\n",
        "\n",
        "          extra_embeddings = self.pred_embedding.unsqueeze(0).repeat(num_missing_nodes, 1)  # Shape: (num_missing_nodes, hidden_dim)\n",
        "\n",
        "          input_embeddings = torch.cat([input_embeddings, extra_embeddings], dim=0)\n",
        "\n",
        "\n",
        "\n",
        "      # Check edge_index validity\n",
        "      num_nodes = input_embeddings.size(1)\n",
        "      if self.edge_index.max() >= num_nodes or self.edge_index.min() < 0:\n",
        "          raise ValueError(\"Invalid node indices in edge_index!\")\n",
        "      # Apply GAT layers\n",
        "      for layer in self.gat_layers:\n",
        "          input_embeddings = layer(input_embeddings, self.edge_index)\n",
        "          input_embeddings = F.relu(input_embeddings)  # Apply non-linearity\n",
        "\n",
        "      return input_embeddings  # Remove batch dimension\n",
        "\n",
        "\n",
        "    def weave(self, text_input_ids, token_type_ids, text_attention_mask):\n",
        "      \"\"\"\n",
        "      Arranges tokens in the format:\n",
        "      [CLS] text [SEP] [V1] [PRED] ... [VL] [PRED], then adds GAT-enhanced embeddings.\n",
        "      \"\"\"\n",
        "      cls_token_id = self.tokenizer.cls_token_id\n",
        "      sep_token_id = self.tokenizer.sep_token_id\n",
        "      batch_size, seq_len = text_input_ids.size()\n",
        "      device = text_input_ids.device\n",
        "\n",
        "      # Add [CLS] and [SEP] around the input tokens\n",
        "      cls_tokens = torch.full((batch_size, 1), cls_token_id, device=device)\n",
        "      sep_tokens = torch.full((batch_size, 1), sep_token_id, device=device)\n",
        "      combined_input_ids = torch.cat([cls_tokens, text_input_ids, sep_tokens], dim=1)\n",
        "      seq_len = combined_input_ids.size(1)\n",
        "\n",
        "      # Extend the attention mask\n",
        "      extended_attention_mask = torch.cat([torch.ones(batch_size, 1, device=device),\n",
        "                                          text_attention_mask,\n",
        "                                          torch.ones(batch_size, 1, device=device)], dim=1)\n",
        "\n",
        "      # Prepare initial input embeddings for the text\n",
        "      input_embeddings = self.text_encoder.embeddings(input_ids=combined_input_ids, token_type_ids=None)\n",
        "\n",
        "      # Get GAT-enhanced embeddings and extend input embeddings\n",
        "      gat_embeddings = self.get_gat_embeddings()  # Shape: (L*2, hidden_dim)\n",
        "      gat_embeddings = gat_embeddings.unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (batch_size, L*2, hidden_dim)\n",
        "      extended_embeddings = torch.cat([input_embeddings, gat_embeddings], dim=1)  # Shape: (batch_size, seq_len + L*2, hidden_dim)\n",
        "\n",
        "      # Extend the attention mask for the GAT embeddings\n",
        "      extended_attention_mask = torch.cat([extended_attention_mask,\n",
        "                                          torch.ones(batch_size, gat_embeddings.size(1), device=device)], dim=1)\n",
        "\n",
        "      # Add final [SEP]\n",
        "      combined_input_ids = torch.cat([combined_input_ids, sep_tokens], dim=1)\n",
        "\n",
        "      # Return the extended embeddings and attention mask\n",
        "      return extended_embeddings, extended_attention_mask\n",
        "\n",
        "\n",
        "    def forward(self, text_input_ids, token_type_ids, text_attention_mask, images):\n",
        "        # Move everything to device\n",
        "        device = text_input_ids.device\n",
        "        self.template_embeddings = self.template_embeddings.to(device)\n",
        "        self.pred_embedding = self.pred_embedding.to(device)\n",
        "        self.label_embeddings = self.label_embeddings.to(device)\n",
        "        self.edge_index = self.edge_index.to(device)\n",
        "\n",
        "        # Prepare embeddings for the combined sequence\n",
        "        batch_size, seq_len = text_input_ids.size()\n",
        "\n",
        "        input_embeddings, extended_attention_mask = self.weave(text_input_ids, token_type_ids, text_attention_mask)\n",
        "        # Forward pass through the encoder\n",
        "        encoder_outputs = self.text_encoder.encoder(\n",
        "            hidden_states=input_embeddings,\n",
        "            attention_mask=self.text_encoder.get_extended_attention_mask(extended_attention_mask, extended_attention_mask.shape, device)\n",
        "        )\n",
        "        sequence_output = encoder_outputs.last_hidden_state\n",
        "\n",
        "        # Extract hidden states of [PRED] tokens\n",
        "        pred_hidden_states = [sequence_output[:, seq_len + self.L + i, :] for i in range(self.L)]\n",
        "\n",
        "\n",
        "        outputs = {}\n",
        "        verbalizer_features = []  # List to collect verbalizer outputs for fusion\n",
        "        for i in range(self.L):\n",
        "            hidden_state = pred_hidden_states[i]\n",
        "            label_indices = [self.label2idx[label] for label in self.labels_per_layer[i]]\n",
        "            logits = torch.matmul(hidden_state, self.label_embeddings[label_indices].t())\n",
        "            verbalizer_features.append(logits)  # Collect logits as verbalizer features\n",
        "            for idx, label in enumerate(self.labels_per_layer[i]):\n",
        "                outputs[label] = logits[:, idx].unsqueeze(-1)\n",
        "\n",
        "\n",
        "        # Stack verbalizer features for fusion\n",
        "        max_num_labels = max([logit.shape[1] for logit in verbalizer_features])  # Find the maximum number of labels\n",
        "        padded_verbalizer_features = [\n",
        "            F.pad(logit, (0, max_num_labels - logit.shape[1]))  # Pad logits to have the same number of labels\n",
        "            for logit in verbalizer_features\n",
        "        ]\n",
        "\n",
        "        # Stack and project verbalizer features\n",
        "        text_features = torch.stack(padded_verbalizer_features, dim=1)  # Shape: (batch_size, L, max_num_labels)\n",
        "        text_features = self.verbalizer_projection(text_features)\n",
        "\n",
        "        # Image encoding\n",
        "        image_features = self.image_encoder(pixel_values=images).last_hidden_state\n",
        "        image_features = self.image_fc(image_features)\n",
        "       # print(image_features.shape, text_features.shape)\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "        text_features = text_features.permute(1, 0, 2)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion_model(text_features, image_features)\n",
        "        fused_features = torch.relu(fused_features)\n",
        "        fused_features = torch.dropout(fused_features, p=0.1, train=self.training)\n",
        "\n",
        "        # Fusion\n",
        "        #text_features = sequence_output[:, 0, :]\n",
        "        ##combined_features = torch.cat([text_features, image_features], dim=-1)\n",
        "        #fused_features = self.fusion_model(text_features, image_features)\n",
        "        #fused_features = torch.relu(fused_features)\n",
        "        ##fused_features = torch.dropout(fused_features, p=0.1, train=self.training)\n",
        "\n",
        "        # Final predictions\n",
        "        final_outputs = {node: self.classifier_dict[node](fused_features) for node in self.labels}\n",
        "\n",
        "        return final_outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvPL2BBIGwOf"
      },
      "source": [
        "## HPT model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnj6IQ8vTy56"
      },
      "source": [
        "Experimental model that does not work :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uza50S-yG0vf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel, BertOnlyMLMHead\n",
        "from transformers.modeling_outputs import MaskedLMOutput\n",
        "from transformers import AutoTokenizer\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx\n",
        "\n",
        "# Assume that G, label_map, depth2label, get_label_index, and get_label_for_index are already defined\n",
        "# If not, define them here or import them\n",
        "\n",
        "# Helper functions if not already defined\n",
        "# get_label_index = ...\n",
        "# get_label_for_index = ...\n",
        "# G = ...\n",
        "# depth2label = ...\n",
        "\n",
        "def multilabel_categorical_crossentropy(y_true, y_pred):\n",
        "    loss_mask = y_true != -100\n",
        "    y_true = y_true.masked_select(loss_mask).view(-1, y_pred.size(-1))\n",
        "    y_pred = y_pred.masked_select(loss_mask).view(-1, y_true.size(-1))\n",
        "    y_pred = (1 - 2 * y_true) * y_pred\n",
        "    y_pred_neg = y_pred - y_true * 1e12\n",
        "    y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
        "    zeros = torch.zeros_like(y_pred[:, :1])\n",
        "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
        "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
        "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
        "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
        "    return (neg_loss + pos_loss).mean()\n",
        "\n",
        "class GraphEmbedding(nn.Module):\n",
        "    def __init__(self, config, embedding, new_embedding, label_map, label_to_index, depth_to_labels, layer=1):\n",
        "        super(GraphEmbedding, self).__init__()\n",
        "        self.label_map = label_map\n",
        "        self.label_to_index = label_to_index\n",
        "        self.depth_to_labels = depth_to_labels\n",
        "\n",
        "        padding_idx = config.pad_token_id\n",
        "        self.num_class = config.num_labels\n",
        "\n",
        "        self.graph = nn.Sequential(*[\n",
        "            GATConv(in_channels=new_embedding.size(-1), out_channels=new_embedding.size(-1)) for _ in range(layer)\n",
        "        ])\n",
        "\n",
        "        self.padding_idx = padding_idx\n",
        "        self.original_embedding = embedding\n",
        "        new_embedding = torch.cat(\n",
        "            [torch.zeros(1, new_embedding.size(-1), device=new_embedding.device, dtype=new_embedding.dtype),\n",
        "             new_embedding], dim=0)\n",
        "        self.new_embedding = nn.Embedding.from_pretrained(new_embedding, freeze=False, padding_idx=0)\n",
        "        self.size = self.original_embedding.num_embeddings + self.new_embedding.num_embeddings - 1\n",
        "        self.depth = self.new_embedding.num_embeddings - 2 - self.num_class\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        def foo():\n",
        "            edge_features = self.new_embedding.weight[1:, :]\n",
        "            edge_features = edge_features[:-1, :]\n",
        "            edge_features = self.graph(edge_features)\n",
        "            edge_features = torch.cat(\n",
        "                [edge_features, self.new_embedding.weight[-1:, :]], dim=0)\n",
        "            return torch.cat([self.original_embedding.weight, edge_features], dim=0)\n",
        "\n",
        "        return foo\n",
        "\n",
        "    @property\n",
        "    def raw_weight(self):\n",
        "        def foo():\n",
        "            return torch.cat([self.original_embedding.weight, self.new_embedding.weight[1:, :]], dim=0)\n",
        "\n",
        "        return foo\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.embedding(x, self.weight(), self.padding_idx)\n",
        "        return x\n",
        "\n",
        "\n",
        "class OutputEmbedding(nn.Module):\n",
        "    def __init__(self, bias):\n",
        "        super(OutputEmbedding, self).__init__()\n",
        "        self.weight = None\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight(), self.bias)\n",
        "\n",
        "\n",
        "class Prompt(BertPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "    def __init__(self, config, graph_type='GAT', layer=1, label_map=None, label_to_index=None, depth_to_labels=None, **kwargs):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained(config, add_pooling_layer=False)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.name_or_path)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.multiclass_bias = nn.Parameter(torch.zeros(self.num_labels, dtype=torch.float32))\n",
        "        bound = 1 / math.sqrt(768)\n",
        "        nn.init.uniform_(self.multiclass_bias, -bound, bound)\n",
        "\n",
        "        self.graph_type = graph_type\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.layer = layer\n",
        "        self.label_map = label_map\n",
        "        self.label_to_index = label_to_index\n",
        "        self.depth_to_labels = depth_to_labels\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    def init_embedding(self):\n",
        "        label_dict = self.label_map\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.name_or_path)\n",
        "        label_emb = []\n",
        "        input_embeds = self.get_input_embeddings()\n",
        "\n",
        "        for label in self.label_map.keys():\n",
        "            encoded = tokenizer.encode(label, add_special_tokens=False)\n",
        "            label_emb.append(\n",
        "                input_embeds.weight.index_select(0, torch.tensor(encoded, device=self.device)).mean(dim=0))\n",
        "\n",
        "        prefix = input_embeds(torch.tensor([tokenizer.mask_token_id],\n",
        "                                           device=self.device, dtype=torch.long))\n",
        "\n",
        "        prompt_embedding = nn.Embedding(3 + 1,\n",
        "                                        input_embeds.weight.size(1), padding_idx=0)\n",
        "\n",
        "        self._init_weights(prompt_embedding)\n",
        "\n",
        "        label_emb = torch.cat(\n",
        "            [torch.stack(label_emb), prompt_embedding.weight[1:, :], prefix], dim=0)\n",
        "\n",
        "        embedding = GraphEmbedding(self.config, input_embeds, label_emb, self.label_map, self.label_to_index,\n",
        "                                     self.depth_to_labels, layer=self.layer)\n",
        "        self.set_input_embeddings(embedding)\n",
        "\n",
        "        output_embeddings = OutputEmbedding(self.get_output_embeddings().bias)\n",
        "        self.set_output_embeddings(output_embeddings)\n",
        "\n",
        "        output_embeddings.weight = embedding.raw_weight\n",
        "        self.vocab_size = output_embeddings.bias.size(0)\n",
        "        output_embeddings.bias.data = nn.functional.pad(\n",
        "            output_embeddings.bias.data,\n",
        "            (\n",
        "                0,\n",
        "                embedding.size - output_embeddings.bias.shape[0],\n",
        "            ),\n",
        "            \"constant\",\n",
        "            0,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            labels=None,\n",
        "            return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        multiclass_pos = input_ids == (self.get_input_embeddings().size - 1)\n",
        "        single_labels = input_ids.masked_fill(multiclass_pos | (input_ids == self.config.pad_token_id), -100)\n",
        "\n",
        "        if self.training:\n",
        "            enable_mask = input_ids < self.tokenizer.vocab_size\n",
        "            random_mask = torch.rand(input_ids.shape, device=input_ids.device) * attention_mask * enable_mask\n",
        "            input_ids = input_ids.masked_fill(random_mask > 0.865, self.tokenizer.mask_token_id)\n",
        "            random_ids = torch.randint_like(input_ids, 104, self.vocab_size)\n",
        "            mlm_mask = random_mask > 0.985\n",
        "            input_ids = input_ids * mlm_mask.logical_not() + random_ids * mlm_mask\n",
        "            mlm_mask = random_mask < 0.85\n",
        "            single_labels = single_labels.masked_fill(mlm_mask, -100)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, prediction_scores.size(-1)),\n",
        "                                      single_labels.view(-1))\n",
        "            multiclass_logits = prediction_scores.masked_select(\n",
        "                multiclass_pos.unsqueeze(-1).expand(-1, -1, prediction_scores.size(-1))).view(-1,\n",
        "                                                                                              prediction_scores.size(\n",
        "                                                                                                  -1))\n",
        "            multiclass_logits = multiclass_logits[:,\n",
        "                                self.vocab_size:self.vocab_size + self.num_labels] + self.multiclass_bias\n",
        "            multiclass_loss = multilabel_categorical_crossentropy(labels.view(-1, self.num_labels), multiclass_logits)\n",
        "            masked_lm_loss += multiclass_loss\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvxRSkqDPHGi"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyXseBzR9KHO"
      },
      "outputs": [],
      "source": [
        "class HierarchyAwareLoss(nn.Module):\n",
        "    def __init__(self, label_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            label_index (dict): Mapping of label names to their indices in the label space.\n",
        "        \"\"\"\n",
        "        super(HierarchyAwareLoss, self).__init__()\n",
        "        self.label_index = label_index\n",
        "\n",
        "    @staticmethod\n",
        "    def multilabel_categorical_crossentropy(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute the multilabel categorical crossentropy loss.\n",
        "\n",
        "        Args:\n",
        "            y_true (torch.Tensor): Ground truth binary labels of shape (batch_size, num_labels).\n",
        "            y_pred (torch.Tensor): Predicted logits of shape (batch_size, num_labels).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value.\n",
        "        \"\"\"\n",
        "        loss_mask = y_true != -100  # Exclude masked values\n",
        "        y_true = y_true.masked_select(loss_mask).view(-1, y_pred.size(-1))\n",
        "        y_pred = y_pred.masked_select(loss_mask).view(-1, y_true.size(-1))\n",
        "        y_pred = (1 - 2 * y_true) * y_pred\n",
        "        y_pred_neg = y_pred - y_true * 1e12\n",
        "        y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
        "        zeros = torch.zeros_like(y_pred[:, :1])\n",
        "        y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
        "        y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
        "        neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
        "        pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
        "        return (neg_loss + pos_loss).mean()\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        \"\"\"\n",
        "        Compute the multi-label categorical crossentropy loss.\n",
        "\n",
        "        Args:\n",
        "            outputs (dict): Dictionary of logits for each label node.\n",
        "            labels (torch.Tensor): Binary ground truth labels of shape (batch_size, num_labels).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value.\n",
        "        \"\"\"\n",
        "        # Combine all logits into a single tensor\n",
        "        logits = torch.stack([outputs[label].squeeze(-1) for label in self.label_index.keys() if label != \"Root\"], dim=1)  # Shape: (batch_size, num_labels)\n",
        "        # Combine labels into a single tensor\n",
        "        labels_combined = torch.stack([labels[:, idx] for idx in self.label_index.values() if idx != 0], dim=1)  # Shape: (batch_size, num_labels)\n",
        "        # Compute the multilabel categorical crossentropy loss\n",
        "        loss = self.multilabel_categorical_crossentropy(labels_combined, logits)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB6wUC6Y9arf"
      },
      "outputs": [],
      "source": [
        "def l1_regularization(model, lambda_l1=1e-6):\n",
        "    l1_loss = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:  # Only include trainable parameters\n",
        "            l1_loss += torch.sum(torch.abs(param))\n",
        "    return lambda_l1 * l1_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjfiijGWG5Ri"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbXr8N60kbm4"
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes, heirarchy):\n",
        "    model = HierarchyAwareClassifier(256, G, layers)\n",
        "    return model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Model file not found!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_loss_function():\n",
        "    return nn.BCEWithLogitsLoss()  # Multi-label classification\n",
        "\n",
        "def find_best_thresholds(predictions, targets):\n",
        "    \"\"\"\n",
        "    Find the best threshold for each class that maximizes the F1 score.\n",
        "\n",
        "    Args:\n",
        "        predictions (torch.Tensor): Predicted probabilities, shape (num_samples, num_classes).\n",
        "        targets (torch.Tensor): Binary ground truth labels, shape (num_samples, num_classes).\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Best threshold for each class.\n",
        "    \"\"\"\n",
        "    num_classes = predictions.shape[1]\n",
        "    best_thresholds = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        # Compute precision, recall, and thresholds\n",
        "        precision, recall, thresholds = precision_recall_curve(targets[:, i].cpu(), predictions[:, i].cpu())\n",
        "\n",
        "        # Compute F1 scores\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)  # Avoid division by zero\n",
        "\n",
        "        # Find the threshold that gives the best F1 score\n",
        "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "        best_thresholds.append(best_threshold)\n",
        "\n",
        "    return best_thresholds\n",
        "\n",
        "def multilabel_accuracy(y_true, y_pred):\n",
        "    # y_true and y_pred are binary arrays (e.g., one-hot encoded)\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def save_model(model):\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def _h_precision_score(labels, preds, hierarchy):\n",
        "    \"\"\"\n",
        "    Calculate hierarchical precision.\n",
        "    Args:\n",
        "        labels (torch.Tensor): Ground truth labels, shape (batch_size, num_labels).\n",
        "        preds (dict): Dictionary of predicted labels for each category.\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph of labels.\n",
        "    Returns:\n",
        "        float: Hierarchical precision score.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_pred = 0\n",
        "\n",
        "    for i in range(labels.shape[0]):  # Iterate over each sample\n",
        "        pred_row = {label: preds[label][i].item() for label in preds.keys()}\n",
        "        label_row = labels[i]\n",
        "\n",
        "        pred_set = _get_active_nodes(pred_row, hierarchy, is_pred=True)\n",
        "        label_set = _get_active_nodes(label_row, hierarchy, is_pred=False)\n",
        "\n",
        "        correct += len(pred_set.intersection(label_set))\n",
        "        total_pred += len(pred_set)\n",
        "\n",
        "    return correct / total_pred if total_pred > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "def _h_recall_score(labels, preds, hierarchy):\n",
        "    \"\"\"\n",
        "    Calculate hierarchical recall.\n",
        "    Args:\n",
        "        labels (torch.Tensor): Ground truth labels, shape (batch_size, num_labels).\n",
        "        preds (dict): Dictionary of predicted labels for each category.\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph of labels.\n",
        "    Returns:\n",
        "        float: Hierarchical recall score.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_label = 0\n",
        "\n",
        "    for i in range(labels.shape[0]):  # Iterate over each sample\n",
        "        pred_row = {label: preds[label][i].item() for label in preds.keys()}\n",
        "        label_row = labels[i]\n",
        "\n",
        "        pred_set = _get_active_nodes(pred_row, hierarchy, is_pred=True)\n",
        "        label_set = _get_active_nodes(label_row, hierarchy, is_pred=False)\n",
        "\n",
        "        correct += len(pred_set.intersection(label_set))\n",
        "        total_label += len(label_set)\n",
        "\n",
        "    return correct / total_label if total_label > 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _h_fbeta_score(labels, preds, hierarchy, beta=1):\n",
        "    \"\"\"\n",
        "    Calculate hierarchical F-beta score.\n",
        "    Args:\n",
        "        labels (torch.Tensor): Ground truth labels, shape (batch_size, num_labels).\n",
        "        preds (dict): Dictionary of predicted labels for each category.\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph of labels.\n",
        "        beta (float): Beta value for F-beta score. Default is 1 for F1 score.\n",
        "    Returns:\n",
        "        float: Hierarchical F-beta score.\n",
        "    \"\"\"\n",
        "    precision = _h_precision_score(labels, preds, hierarchy)\n",
        "    recall = _h_recall_score(labels, preds, hierarchy)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "\n",
        "    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "\n",
        "\n",
        "\n",
        "def _get_active_nodes(example, hierarchy, is_pred=False):\n",
        "    \"\"\"\n",
        "    Get active nodes (with value 1) from an example and their ancestors.\n",
        "    Args:\n",
        "        example (torch.Tensor or dict): Binary vector or prediction dictionary for one example.\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph of labels.\n",
        "        is_pred (bool): Whether the example is from predictions or labels.\n",
        "    Returns:\n",
        "        set: Active nodes (names) and their ancestors for predictions; labels directly for ground truth.\n",
        "    \"\"\"\n",
        "    active_nodes = set()\n",
        "\n",
        "    if is_pred:\n",
        "        # Example is a dictionary of predictions\n",
        "        for label, value in example.items():\n",
        "            if value == 1:  # Active prediction\n",
        "                active_nodes.update(_get_ancestors(label, hierarchy))\n",
        "    else:\n",
        "        # Example is a tensor of ground truth labels\n",
        "        for idx in torch.nonzero(example, as_tuple=False).squeeze(1).tolist():\n",
        "            label = get_label_for_index(idx)  # Convert index to label name\n",
        "            active_nodes.add(label)  # Use labels directly as all ancestors are already included\n",
        "\n",
        "    return active_nodes\n",
        "\n",
        "\n",
        "\n",
        "def _get_ancestors(node, hierarchy):\n",
        "    \"\"\"\n",
        "    Get all ancestors of a node, including the node itself.\n",
        "    Args:\n",
        "        node (int): Node index to retrieve ancestors for.\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph.\n",
        "    Returns:\n",
        "        set: Set of ancestor nodes (indices).\n",
        "    \"\"\"\n",
        "    ancestors = set(nx.ancestors(hierarchy, node))\n",
        "    ancestors.add(node)\n",
        "    return ancestors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_hierarchical_metrics(labels, preds, hierarchy):\n",
        "    \"\"\"\n",
        "    Compute hierarchical precision, recall, and F1 score.\n",
        "    Args:\n",
        "        labels (torch.Tensor): Ground truth labels, shape (batch_size, num_labels).\n",
        "        preds (torch.Tensor): Predicted labels, shape (batch_size, num_labels).\n",
        "        hierarchy (networkx.DiGraph): Hierarchy graph of labels.\n",
        "    Returns:\n",
        "        dict: Hierarchical precision, recall, and F1 score.\n",
        "    \"\"\"\n",
        "    hierarchical_precision = _h_precision_score(labels, preds, hierarchy)\n",
        "    hierarchical_recall = _h_recall_score(labels, preds, hierarchy)\n",
        "    hierarchical_f1 = _h_fbeta_score(labels, preds, hierarchy)\n",
        "\n",
        "    return {\n",
        "        \"hierarchical_precision\": hierarchical_precision,\n",
        "        \"hierarchical_recall\": hierarchical_recall,\n",
        "        \"hierarchical_f1\": hierarchical_f1,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8qltmYHUAc3"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGcLOlr3G9ZO"
      },
      "source": [
        "## Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-Q2aq1kkdhC"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, device, loss_fn):\n",
        "    if isinstance(device, str):\n",
        "        device = torch.device(device)\n",
        "    global label_map\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = {node: [] for node in model.hierarchy.nodes if node != \"Root\"}  # Initialize probabilities storage\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            text_input_ids, token_type_ids, attention_masks, images, labels = [b.to(device) for b in batch]\n",
        "            # check if labels has a 1\n",
        "            non_zero_mask = torch.any(labels != 0, dim=1)\n",
        "\n",
        "            # Filter out the zero-only rows from all batch components\n",
        "            text_input_ids = text_input_ids[non_zero_mask]\n",
        "            token_type_ids = token_type_ids[non_zero_mask]\n",
        "            attention_masks = attention_masks[non_zero_mask]\n",
        "            images = images[non_zero_mask]\n",
        "            labels = labels[non_zero_mask]\n",
        "            # Forward pass\n",
        "            outputs = model(text_input_ids, token_type_ids, attention_masks, images)\n",
        "            probs = {node: torch.sigmoid(outputs[node].squeeze(-1)) for node in outputs.keys()}\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Store probabilities and ground truth labels\n",
        "            all_labels.append(labels.cpu())\n",
        "            for node in probs.keys():\n",
        "                all_probs[node].append(probs[node].cpu())\n",
        "\n",
        "    # Aggregate labels and probabilities\n",
        "    all_labels = torch.cat(all_labels, dim=0)  # Ground truth labels\n",
        "    all_probs = {node: torch.cat(all_probs[node], dim=0) for node in all_probs.keys()}\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    thresholds = find_best_thresholds(torch.stack(list(all_probs.values())).T, all_labels)\n",
        "    threshold_dict = {node: threshold for node, threshold in zip(all_probs.keys(), thresholds)}\n",
        "    all_preds = {node: (all_probs[node] > threshold_dict[node]).int() for node in all_probs.keys()}\n",
        "\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    metrics = {}\n",
        "    precision_sum = 0\n",
        "    recall_sum = 0\n",
        "    f1_sum = 0\n",
        "    total_samples = 0\n",
        "    for node in all_probs.keys():\n",
        "        idx = label_map[node]\n",
        "        precision = precision_score(all_labels[:, idx], all_preds[node], zero_division=0)\n",
        "        recall = recall_score(all_labels[:, idx], all_preds[node])\n",
        "        f1 = f1_score(all_labels[:, idx], all_preds[node])\n",
        "        metrics[node] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "        num_samples = int(all_labels[:, idx].sum())\n",
        "        precision_sum += precision * num_samples\n",
        "        recall_sum += recall * num_samples\n",
        "        f1 = f1_score(all_labels[:, idx], all_preds[node])\n",
        "        f1_sum += f1 * num_samples\n",
        "        total_samples += num_samples\n",
        "\n",
        "    # Calculate overall weighted metrics\n",
        "    if total_samples > 0:\n",
        "        overall_metrics = {\n",
        "            \"precision\": precision_sum / total_samples,\n",
        "            \"recall\": recall_sum / total_samples,\n",
        "            \"f1\": f1_sum / total_samples,\n",
        "        }\n",
        "    else:\n",
        "        overall_metrics = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
        "\n",
        "    # Compute hierarchical metrics\n",
        "    hierarchical_metrics = compute_hierarchical_metrics(all_labels, all_preds, model.hierarchy)\n",
        "\n",
        "    # Log metrics\n",
        "    print(f\"Validation Loss: {total_loss / len(val_loader):.4f}\")\n",
        "    for node, metric in metrics.items():\n",
        "        print(f\"{node}: Precision: {metric['precision']:.4f}, Recall: {metric['recall']:.4f}, F1: {metric['f1']:.4f}\")\n",
        "    print(\"<------------------------------------>\")\n",
        "    print(f\"Overall Metrics: Precision: {overall_metrics['precision']:.4f}, Recall: {overall_metrics['recall']:.4f}, F1: {overall_metrics['f1']:.4f}\")\n",
        "    print(f\"Hierarchical Metrics: {hierarchical_metrics}\")\n",
        "    print(\"<------------------------------------>\")\n",
        "    return total_loss / len(val_loader), hierarchical_metrics, overall_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VThC4suHAWs"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eteGmH1i2ROQ"
      },
      "source": [
        "### Base train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZX2JEuwkgB8"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, epochs=NUM_EPOCHS):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define optimizer, scheduler, and loss function\n",
        "    optimizer = Adam(model.parameters(), lr=5e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.9)\n",
        "    for param in model.fusion_model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    loss_list = []\n",
        "    metrics_list = []\n",
        "    metrics_list2 = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        train_loader = tqdm(all_train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "        for batch in train_loader:\n",
        "            text_input_ids, token_type_ids, attention_masks, images, labels = [b.to(device) for b in batch]\n",
        "            non_zero_mask = torch.any(labels != 0, dim=1)\n",
        "            # Filter out the zero-only rows from all batch components\n",
        "            text_input_ids = text_input_ids[non_zero_mask]\n",
        "            token_type_ids = token_type_ids[non_zero_mask]\n",
        "            attention_masks = attention_masks[non_zero_mask]\n",
        "            images = images[non_zero_mask]\n",
        "            labels = labels[non_zero_mask]\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(text_input_ids, token_type_ids, attention_masks, images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            #loss += l1_regularization(model)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_loader.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_train_loss = train_loss / len(all_train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        val_loss, metrics1, metrics2 = validate(model, all_val_loader, device, loss_fn)\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        loss_list.append(avg_train_loss)\n",
        "        metrics_list.append(metrics1)\n",
        "        metrics_list2.append(metrics2)\n",
        "        # Adjust learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    return loss_list, metrics_list, metrics_list2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGAjTo1H2TsV"
      },
      "source": [
        "### Heirarchy aware train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmujpZWrHDiw"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yINnxG2nEBAs",
        "outputId": "246e938a-4f26-4157-e953-d567321c0014"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "HierarchyAwareClassifier(\n",
              "  (text_encoder): DebertaV2Model(\n",
              "    (embeddings): DebertaV2Embeddings(\n",
              "      (word_embeddings): Embedding(128005, 768, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaV2Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DebertaV2Layer(\n",
              "          (attention): DebertaV2Attention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaV2SelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaV2Intermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaV2Output(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (gat_layers): ModuleList(\n",
              "    (0-2): 3 x GATConv(768, 768, heads=1)\n",
              "  )\n",
              "  (fusion_model): TransformerFusion(\n",
              "    (text_transformer): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (image_transformer): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cross_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "    )\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=2304, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.3, inplace=False)\n",
              "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    )\n",
              "    (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (image_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (image_encoder): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (image_fc): Sequential(\n",
              "    (0): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (fusion_fc): Linear(in_features=1536, out_features=768, bias=True)\n",
              "  (classifier_dict): ModuleDict(\n",
              "    (Logos): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Repetition): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Obfuscation, Intentional vagueness, Confusion): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Reasoning): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Justification): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Slogans): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Bandwagon): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Appeal to authority): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Flag-waving): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Appeal to fear/prejudice): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Simplification): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Causal Oversimplification): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Black-and-white Fallacy/Dictatorship): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Thought-terminating cliché): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Distraction): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Misrepresentation of Someone's Position (Straw Man)): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Presenting Irrelevant Data (Red Herring)): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Whataboutism): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Ethos): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Glittering generalities (Virtue)): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Ad Hominem): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Transfer): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Doubt): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Name calling/Labeling): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Smears): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Reductio ad hitlerum): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Pathos): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Exaggeration/Minimisation): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Loaded Language): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (Appeal to (Strong) Emotions): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              "  (verbalizer_projection): Linear(in_features=14, out_features=768, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = get_model(22,G)\n",
        "#model = Prompt.from_pretrained(\"bert-base-uncased\", graph_type='GAT', layer=3, label_map=label_map, label_to_index=get_label_for_index, index_to_label=None, depth_to_labels=depth_to_label)\n",
        "model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNUfhrzaMf2N",
        "outputId": "3b3723c9-3c78-40e5-9a19-7ee187c15966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.0300\n",
            "Logos: Precision: 0.5571, Recall: 1.0000, F1: 0.7156\n",
            "Repetition: Precision: 0.0448, Recall: 0.9565, F1: 0.0856\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.0125, Recall: 0.2000, F1: 0.0235\n",
            "Reasoning: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Justification: Precision: 0.4444, Recall: 0.0221, F1: 0.0421\n",
            "Slogans: Precision: 0.1556, Recall: 0.1346, F1: 0.1443\n",
            "Bandwagon: Precision: 0.0175, Recall: 0.8750, F1: 0.0342\n",
            "Appeal to authority: Precision: 0.2667, Recall: 0.0606, F1: 0.0988\n",
            "Flag-waving: Precision: 0.1659, Recall: 0.6102, F1: 0.2609\n",
            "Appeal to fear/prejudice: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Simplification: Precision: 0.2609, Recall: 0.0594, F1: 0.0968\n",
            "Causal Oversimplification: Precision: 0.0427, Recall: 0.9545, F1: 0.0817\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.2222, Recall: 0.0364, F1: 0.0625\n",
            "Thought-terminating cliché: Precision: 0.0754, Recall: 0.9737, F1: 0.1399\n",
            "Distraction: Precision: 0.1111, Recall: 0.0294, F1: 0.0465\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0119, Recall: 0.8000, F1: 0.0235\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Whataboutism: Precision: 0.1053, Recall: 0.0800, F1: 0.0909\n",
            "Ethos: Precision: 0.7800, Recall: 0.9770, F1: 0.8675\n",
            "Glittering generalities (Virtue): Precision: 0.0909, Recall: 0.0200, F1: 0.0328\n",
            "Ad Hominem: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Transfer: Precision: 0.2249, Recall: 0.3220, F1: 0.2648\n",
            "Doubt: Precision: 0.0550, Recall: 0.9643, F1: 0.1040\n",
            "Name calling/Labeling: Precision: 0.2394, Recall: 0.9672, F1: 0.3837\n",
            "Smears: Precision: 0.5091, Recall: 0.9767, F1: 0.6693\n",
            "Reductio ad hitlerum: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Pathos: Precision: 0.3449, Recall: 0.9769, F1: 0.5098\n",
            "Exaggeration/Minimisation: Precision: 0.0591, Recall: 0.9667, F1: 0.1113\n",
            "Loaded Language: Precision: 0.2913, Recall: 0.7704, F1: 0.4228\n",
            "Appeal to (Strong) Emotions: Precision: 0.0435, Recall: 0.0370, F1: 0.0400\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.3307, Recall: 0.5574, F1: 0.3615\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.24405014874628134, 'hierarchical_recall': 0.8244795405599425, 'hierarchical_f1': 0.37661911788817837}\n",
            "<------------------------------------>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5.029999074481783,\n",
              " {'hierarchical_precision': 0.24405014874628134,\n",
              "  'hierarchical_recall': 0.8244795405599425,\n",
              "  'hierarchical_f1': 0.37661911788817837},\n",
              " {'precision': 0.3306728372535554,\n",
              "  'recall': 0.5574300071787509,\n",
              "  'f1': 0.361451866177274})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validate(model, all_val_loader, torch.device(\"cuda\"), HierarchyAwareLoss(label_map))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jei2RBuXkkJB",
        "outputId": "e638417d-7ef1-46c0-bb7a-31c0ac8e284a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/292 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 4.4823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.4372\n",
            "Logos: Precision: 0.5517, Recall: 0.9784, F1: 0.7056\n",
            "Repetition: Precision: 0.0460, Recall: 0.9565, F1: 0.0878\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.0556, Recall: 0.2000, F1: 0.0870\n",
            "Reasoning: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Justification: Precision: 0.3602, Recall: 0.9890, F1: 0.5280\n",
            "Slogans: Precision: 0.1130, Recall: 0.8846, F1: 0.2004\n",
            "Bandwagon: Precision: 0.0138, Recall: 0.2500, F1: 0.0261\n",
            "Appeal to authority: Precision: 0.1282, Recall: 0.9091, F1: 0.2247\n",
            "Flag-waving: Precision: 0.1330, Recall: 0.4915, F1: 0.2094\n",
            "Appeal to fear/prejudice: Precision: 0.0217, Recall: 0.0294, F1: 0.0250\n",
            "Simplification: Precision: 0.2340, Recall: 0.3267, F1: 0.2727\n",
            "Causal Oversimplification: Precision: 0.0365, Recall: 0.3636, F1: 0.0664\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.2000, Recall: 0.0364, F1: 0.0615\n",
            "Thought-terminating cliché: Precision: 0.0709, Recall: 0.5000, F1: 0.1242\n",
            "Distraction: Precision: 0.0528, Recall: 0.4412, F1: 0.0943\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0105, Recall: 1.0000, F1: 0.0208\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Whataboutism: Precision: 0.0305, Recall: 0.3200, F1: 0.0557\n",
            "Ethos: Precision: 0.7800, Recall: 0.9770, F1: 0.8675\n",
            "Glittering generalities (Virtue): Precision: 0.1014, Recall: 1.0000, F1: 0.1842\n",
            "Ad Hominem: Precision: 0.6477, Recall: 0.9755, F1: 0.7785\n",
            "Transfer: Precision: 0.2257, Recall: 0.8475, F1: 0.3565\n",
            "Doubt: Precision: 0.0476, Recall: 0.7143, F1: 0.0893\n",
            "Name calling/Labeling: Precision: 0.2522, Recall: 0.7131, F1: 0.3726\n",
            "Smears: Precision: 0.5120, Recall: 0.9922, F1: 0.6755\n",
            "Reductio ad hitlerum: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Pathos: Precision: 0.4239, Recall: 0.2254, F1: 0.2943\n",
            "Exaggeration/Minimisation: Precision: 0.0608, Recall: 0.6667, F1: 0.1114\n",
            "Loaded Language: Precision: 0.2675, Recall: 0.7630, F1: 0.3962\n",
            "Appeal to (Strong) Emotions: Precision: 0.0544, Recall: 1.0000, F1: 0.1033\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.3980, Recall: 0.7552, F1: 0.4852\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.2266023559966608, 'hierarchical_recall': 0.8768844221105527, 'hierarchical_f1': 0.36013857153386897}\n",
            "<------------------------------------>\n",
            "Epoch 1/15, Validation Loss: 4.4372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2/5:   0%|          | 0/292 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Training Loss: 4.4213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.4316\n",
            "Logos: Precision: 0.7778, Recall: 0.0252, F1: 0.0488\n",
            "Repetition: Precision: 0.0434, Recall: 0.9130, F1: 0.0828\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.0714, Recall: 0.2000, F1: 0.1053\n",
            "Reasoning: Precision: 0.3333, Recall: 0.0155, F1: 0.0296\n",
            "Justification: Precision: 0.3592, Recall: 0.4862, F1: 0.4131\n",
            "Slogans: Precision: 0.1062, Recall: 0.9808, F1: 0.1917\n",
            "Bandwagon: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Appeal to authority: Precision: 0.2000, Recall: 0.0455, F1: 0.0741\n",
            "Flag-waving: Precision: 0.1259, Recall: 0.8983, F1: 0.2208\n",
            "Appeal to fear/prejudice: Precision: 0.0707, Recall: 0.7941, F1: 0.1298\n",
            "Simplification: Precision: 0.2261, Recall: 0.5842, F1: 0.3260\n",
            "Causal Oversimplification: Precision: 0.0407, Recall: 0.8182, F1: 0.0776\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.2174, Recall: 0.0909, F1: 0.1282\n",
            "Thought-terminating cliché: Precision: 0.0751, Recall: 0.9737, F1: 0.1394\n",
            "Distraction: Precision: 0.0698, Recall: 1.0000, F1: 0.1305\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0069, Recall: 0.2000, F1: 0.0133\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0141, Recall: 0.5000, F1: 0.0274\n",
            "Whataboutism: Precision: 0.0909, Recall: 0.0400, F1: 0.0556\n",
            "Ethos: Precision: 0.7800, Recall: 0.9770, F1: 0.8675\n",
            "Glittering generalities (Virtue): Precision: 0.1025, Recall: 0.9000, F1: 0.1840\n",
            "Ad Hominem: Precision: 0.6111, Recall: 0.0337, F1: 0.0640\n",
            "Transfer: Precision: 0.2432, Recall: 0.9831, F1: 0.3899\n",
            "Doubt: Precision: 0.0465, Recall: 0.0714, F1: 0.0563\n",
            "Name calling/Labeling: Precision: 0.2451, Recall: 0.9180, F1: 0.3869\n",
            "Smears: Precision: 0.5074, Recall: 0.8016, F1: 0.6214\n",
            "Reductio ad hitlerum: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Pathos: Precision: 0.3447, Recall: 0.9942, F1: 0.5119\n",
            "Exaggeration/Minimisation: Precision: 0.0714, Recall: 0.0333, F1: 0.0455\n",
            "Loaded Language: Precision: 0.2612, Recall: 0.9037, F1: 0.4053\n",
            "Appeal to (Strong) Emotions: Precision: 0.0500, Recall: 0.1481, F1: 0.0748\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.4294, Recall: 0.5686, F1: 0.3414\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.24566532258064516, 'hierarchical_recall': 0.8747307968413496, 'hierarchical_f1': 0.38359830001574063}\n",
            "<------------------------------------>\n",
            "Epoch 2/15, Validation Loss: 4.4316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3/5:   0%|          | 0/292 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Training Loss: 4.4112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.4226\n",
            "Logos: Precision: 0.7778, Recall: 0.0252, F1: 0.0488\n",
            "Repetition: Precision: 0.0477, Recall: 0.8261, F1: 0.0903\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.0092, Recall: 0.2000, F1: 0.0175\n",
            "Reasoning: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Justification: Precision: 0.3291, Recall: 0.1436, F1: 0.2000\n",
            "Slogans: Precision: 0.1017, Recall: 0.1154, F1: 0.1081\n",
            "Bandwagon: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Appeal to authority: Precision: 0.1408, Recall: 0.4394, F1: 0.2132\n",
            "Flag-waving: Precision: 0.1134, Recall: 0.9492, F1: 0.2025\n",
            "Appeal to fear/prejudice: Precision: 0.0681, Recall: 1.0000, F1: 0.1276\n",
            "Simplification: Precision: 0.3000, Recall: 0.0297, F1: 0.0541\n",
            "Causal Oversimplification: Precision: 0.0473, Recall: 0.6364, F1: 0.0881\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.1111, Recall: 0.0364, F1: 0.0548\n",
            "Thought-terminating cliché: Precision: 0.1212, Recall: 0.1053, F1: 0.1127\n",
            "Distraction: Precision: 0.1111, Recall: 0.0294, F1: 0.0465\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0102, Recall: 1.0000, F1: 0.0201\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0094, Recall: 0.5000, F1: 0.0185\n",
            "Whataboutism: Precision: 0.0539, Recall: 0.4400, F1: 0.0961\n",
            "Ethos: Precision: 1.0000, Recall: 0.0230, F1: 0.0449\n",
            "Glittering generalities (Virtue): Precision: 0.1006, Recall: 1.0000, F1: 0.1828\n",
            "Ad Hominem: Precision: 0.8889, Recall: 0.0245, F1: 0.0478\n",
            "Transfer: Precision: 0.2365, Recall: 1.0000, F1: 0.3825\n",
            "Doubt: Precision: 0.0533, Recall: 0.1429, F1: 0.0777\n",
            "Name calling/Labeling: Precision: 0.2429, Recall: 0.9754, F1: 0.3889\n",
            "Smears: Precision: 0.4954, Recall: 0.4163, F1: 0.4524\n",
            "Reductio ad hitlerum: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Pathos: Precision: 0.3404, Recall: 0.1850, F1: 0.2397\n",
            "Exaggeration/Minimisation: Precision: 0.0667, Recall: 0.1000, F1: 0.0800\n",
            "Loaded Language: Precision: 0.2500, Recall: 0.1333, F1: 0.1739\n",
            "Appeal to (Strong) Emotions: Precision: 0.0542, Recall: 1.0000, F1: 0.1029\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.4727, Recall: 0.2566, F1: 0.1544\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.22597589865073642, 'hierarchical_recall': 0.7875089734386217, 'hierarchical_f1': 0.35118047218887555}\n",
            "<------------------------------------>\n",
            "Epoch 3/15, Validation Loss: 4.4226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4/5:   0%|          | 0/292 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Training Loss: 4.4024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.4260\n",
            "Logos: Precision: 0.7333, Recall: 0.0396, F1: 0.0751\n",
            "Repetition: Precision: 0.0444, Recall: 0.2609, F1: 0.0759\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.0222, Recall: 0.2000, F1: 0.0400\n",
            "Reasoning: Precision: 0.2000, Recall: 0.0078, F1: 0.0149\n",
            "Justification: Precision: 0.3434, Recall: 0.1878, F1: 0.2429\n",
            "Slogans: Precision: 0.1037, Recall: 0.9808, F1: 0.1875\n",
            "Bandwagon: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Appeal to authority: Precision: 0.1365, Recall: 0.5606, F1: 0.2196\n",
            "Flag-waving: Precision: 0.3333, Recall: 0.0508, F1: 0.0882\n",
            "Appeal to fear/prejudice: Precision: 0.0671, Recall: 0.3235, F1: 0.1111\n",
            "Simplification: Precision: 0.3030, Recall: 0.0990, F1: 0.1493\n",
            "Causal Oversimplification: Precision: 0.0385, Recall: 0.0909, F1: 0.0541\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.1053, Recall: 0.0364, F1: 0.0541\n",
            "Thought-terminating cliché: Precision: 0.0760, Recall: 0.6579, F1: 0.1362\n",
            "Distraction: Precision: 0.0638, Recall: 0.1765, F1: 0.0938\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0083, Recall: 0.2000, F1: 0.0159\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Whataboutism: Precision: 0.0521, Recall: 0.8800, F1: 0.0984\n",
            "Ethos: Precision: 0.7458, Recall: 0.3367, F1: 0.4640\n",
            "Glittering generalities (Virtue): Precision: 0.1058, Recall: 0.4400, F1: 0.1705\n",
            "Ad Hominem: Precision: 0.8889, Recall: 0.0245, F1: 0.0478\n",
            "Transfer: Precision: 0.2349, Recall: 0.9915, F1: 0.3799\n",
            "Doubt: Precision: 0.1111, Recall: 0.0357, F1: 0.0541\n",
            "Name calling/Labeling: Precision: 0.2425, Recall: 0.9918, F1: 0.3897\n",
            "Smears: Precision: 0.7692, Recall: 0.0389, F1: 0.0741\n",
            "Reductio ad hitlerum: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Pathos: Precision: 0.3429, Recall: 0.8960, F1: 0.4960\n",
            "Exaggeration/Minimisation: Precision: 0.0411, Recall: 0.1000, F1: 0.0583\n",
            "Loaded Language: Precision: 0.2672, Recall: 0.9481, F1: 0.4169\n",
            "Appeal to (Strong) Emotions: Precision: 0.0542, Recall: 1.0000, F1: 0.1029\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.4726, Recall: 0.3399, F1: 0.2145\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.2510206462148606, 'hierarchical_recall': 0.7724335965541995, 'hierarchical_f1': 0.37890659389030723}\n",
            "<------------------------------------>\n",
            "Epoch 4/15, Validation Loss: 4.4260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 5/5:   0%|          | 0/292 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Training Loss: 4.4025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.4231\n",
            "Logos: Precision: 0.5558, Recall: 0.9676, F1: 0.7060\n",
            "Repetition: Precision: 0.0648, Recall: 0.8261, F1: 0.1203\n",
            "Obfuscation, Intentional vagueness, Confusion: Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
            "Reasoning: Precision: 0.5000, Recall: 0.0233, F1: 0.0444\n",
            "Justification: Precision: 0.3621, Recall: 0.8564, F1: 0.5090\n",
            "Slogans: Precision: 0.1683, Recall: 0.3269, F1: 0.2222\n",
            "Bandwagon: Precision: 0.0085, Recall: 0.1250, F1: 0.0160\n",
            "Appeal to authority: Precision: 0.1444, Recall: 0.4091, F1: 0.2134\n",
            "Flag-waving: Precision: 0.1294, Recall: 0.3729, F1: 0.1921\n",
            "Appeal to fear/prejudice: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Simplification: Precision: 0.2073, Recall: 0.1683, F1: 0.1858\n",
            "Causal Oversimplification: Precision: 0.1000, Recall: 0.0455, F1: 0.0625\n",
            "Black-and-white Fallacy/Dictatorship: Precision: 0.1319, Recall: 0.5636, F1: 0.2138\n",
            "Thought-terminating cliché: Precision: 0.0755, Recall: 0.1053, F1: 0.0879\n",
            "Distraction: Precision: 0.1111, Recall: 0.0294, F1: 0.0465\n",
            "Misrepresentation of Someone's Position (Straw Man): Precision: 0.0112, Recall: 1.0000, F1: 0.0221\n",
            "Presenting Irrelevant Data (Red Herring): Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
            "Whataboutism: Precision: 0.0545, Recall: 0.1200, F1: 0.0750\n",
            "Ethos: Precision: 0.8462, Recall: 0.0281, F1: 0.0543\n",
            "Glittering generalities (Virtue): Precision: 0.1140, Recall: 0.2600, F1: 0.1585\n",
            "Ad Hominem: Precision: 0.8889, Recall: 0.0245, F1: 0.0478\n",
            "Transfer: Precision: 0.2368, Recall: 0.9915, F1: 0.3824\n",
            "Doubt: Precision: 0.0556, Recall: 0.9643, F1: 0.1051\n",
            "Name calling/Labeling: Precision: 0.1935, Recall: 0.0492, F1: 0.0784\n",
            "Smears: Precision: 0.5071, Recall: 0.9689, F1: 0.6658\n",
            "Reductio ad hitlerum: Precision: 0.1111, Recall: 0.1111, F1: 0.1111\n",
            "Pathos: Precision: 0.3911, Recall: 0.5087, F1: 0.4422\n",
            "Exaggeration/Minimisation: Precision: 0.0769, Recall: 0.0333, F1: 0.0465\n",
            "Loaded Language: Precision: 0.2685, Recall: 0.9926, F1: 0.4227\n",
            "Appeal to (Strong) Emotions: Precision: 0.0625, Recall: 0.5926, F1: 0.1131\n",
            "<------------------------------------>\n",
            "Overall Metrics: Precision: 0.4560, Recall: 0.4476, F1: 0.2821\n",
            "Hierarchical Metrics: {'hierarchical_precision': 0.27153855554197726, 'hierarchical_recall': 0.797559224694903, 'hierarchical_f1': 0.40514176315069744}\n",
            "<------------------------------------>\n",
            "Epoch 5/15, Validation Loss: 4.4231\n"
          ]
        }
      ],
      "source": [
        "h_metrics_list = []\n",
        "metrics2_list = []\n",
        "loss_list = []\n",
        "for param in model.text_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.image_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "for i in range(1):\n",
        "  loss, metrics_list, metrics_list2 = train(model,HierarchyAwareLoss(label_map),  5)\n",
        "  h_metrics_list.extend([\n",
        "        {\n",
        "            \"precision\": metrics[\"hierarchical_precision\"],\n",
        "            \"recall\": metrics[\"hierarchical_recall\"],\n",
        "            \"f1\": metrics[\"hierarchical_f1\"]\n",
        "        } for metrics in metrics_list\n",
        "  ])\n",
        "\n",
        "  # Append loss and metrics2 for this iteration\n",
        "  loss_list.extend(loss)\n",
        "  metrics2_list.extend(metrics_list2)\n",
        "save_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxnnnqkWmkaG"
      },
      "outputs": [],
      "source": [
        "save_model(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gP_IKa1116Iw",
        "yvxRSkqDPHGi",
        "TjfiijGWG5Ri",
        "sGcLOlr3G9ZO",
        "eteGmH1i2ROQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
